{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ecom-user-churn-data.csv')\n",
    "\n",
    "df = df.drop(['visitorid', 'int_cat15_n'], axis = 1)\n",
    "X = df.drop(['target_class'], axis=1)\n",
    "y = df['target_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=1010, \n",
    "                                                    stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=1010, \n",
    "                                                    stratify=y_train)\n",
    "\n",
    "# train data\n",
    "X_train = torch.tensor(X_train.values)\n",
    "y_train = torch.tensor(y_train.values)\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# validation data\n",
    "X_val = torch.tensor(X_val.values)\n",
    "y_val = torch.tensor(y_val.values)\n",
    "dataset_val = TensorDataset(X_val, y_val)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "X_test = torch.tensor(X_test.values)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = torch.nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size_1),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_size_2, output_size),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(1, 20, 10, 2)\n",
    "# summary(model, (1,))\n",
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 1.972, val loss: 0.327, train acc: 0.851, val acc: 3.405\n",
      "epoch: 2, train loss: 0.363, val loss: 0.325, train acc: 0.855, val acc: 3.419\n",
      "epoch: 3, train loss: 0.361, val loss: 0.326, train acc: 0.856, val acc: 3.425\n",
      "epoch: 4, train loss: 0.360, val loss: 0.335, train acc: 0.855, val acc: 3.420\n",
      "epoch: 5, train loss: 0.360, val loss: 0.326, train acc: 0.855, val acc: 3.419\n",
      "epoch: 6, train loss: 0.362, val loss: 0.332, train acc: 0.856, val acc: 3.425\n",
      "epoch: 7, train loss: 0.363, val loss: 0.331, train acc: 0.857, val acc: 3.426\n",
      "epoch: 8, train loss: 0.359, val loss: 0.324, train acc: 0.855, val acc: 3.421\n",
      "epoch: 9, train loss: 0.363, val loss: 0.324, train acc: 0.854, val acc: 3.415\n",
      "epoch: 10, train loss: 0.364, val loss: 0.328, train acc: 0.853, val acc: 3.413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.9724323920009634,\n",
       "  0.36336777269651616,\n",
       "  0.361382598198856,\n",
       "  0.35989747278958184,\n",
       "  0.3600430758080528,\n",
       "  0.3616544528852535,\n",
       "  0.36294015051349054,\n",
       "  0.35947065861730637,\n",
       "  0.3630663968999929,\n",
       "  0.36424723732037634],\n",
       " 'valid_loss': [0.3270792544265337,\n",
       "  0.3251202248885662,\n",
       "  0.3256573750154127,\n",
       "  0.3348731731500807,\n",
       "  0.32600902497202533,\n",
       "  0.33204614672856997,\n",
       "  0.33095874363863015,\n",
       "  0.32365559503624713,\n",
       "  0.3242533565793611,\n",
       "  0.3277713791290416],\n",
       " 'train_accuracy': [0.8512058565510979,\n",
       "  0.8547068574949156,\n",
       "  0.8562358379175391,\n",
       "  0.8550666174556636,\n",
       "  0.854865084531941,\n",
       "  0.8561642195416402,\n",
       "  0.8565539591297319,\n",
       "  0.8551299086288561,\n",
       "  0.8537491621284545,\n",
       "  0.8533460976956766],\n",
       " 'valid_accuracy': [3.4048234262043917,\n",
       "  3.4188274299796624,\n",
       "  3.4249433516701564,\n",
       "  3.4202664698226544,\n",
       "  3.419460338127764,\n",
       "  3.424656878166561,\n",
       "  3.4262158365189275,\n",
       "  3.4205196345154243,\n",
       "  3.414996648513818,\n",
       "  3.4133843907827064]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainer(model, criterion, optimizer, dataloader_train, dataloader_val, epochs=5, patience=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    per_epoch_train_loss = []\n",
    "    per_epoch_val_loss = []\n",
    "    per_epoch_train_accuracy = []\n",
    "    per_epoch_val_accuracy = []\n",
    "    consec_increases = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_train_loss = 0\n",
    "        batch_train_acc = 0\n",
    "        \n",
    "        # training\n",
    "        for X_train, y_train in dataloader_train:\n",
    "            X_train = X_train.float()\n",
    "            y_train = y_train.long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = model(X_train)\n",
    "            y_hat_labels = torch.argmax(y_hat, dim=1)\n",
    "\n",
    "            loss = criterion(y_hat, y_train)\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_train_loss += loss.item()\n",
    "            batch_train_acc += (y_hat_labels == y_train).type(torch.float32).mean().item()\n",
    "        \n",
    "        # per batch loss & accuracy\n",
    "        avg_batch_train_loss = batch_train_loss / len(dataloader_train)\n",
    "        per_epoch_train_loss.append(avg_batch_train_loss)\n",
    "        \n",
    "        avg_batch_train_acc = batch_train_acc / len(dataloader_train)\n",
    "        per_epoch_train_accuracy.append(avg_batch_train_acc)\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_val_loss = 0\n",
    "            batch_val_acc = 0\n",
    "\n",
    "            for X_val, y_val in dataloader_val:\n",
    "                X_val = X_val.float()\n",
    "                y_val = y_val.long()\n",
    "\n",
    "                y_hat = model(X_val)\n",
    "                y_hat_labels = torch.argmax(y_hat, dim=1)\n",
    "                loss = criterion(y_hat, y_val)\n",
    "                \n",
    "                batch_val_loss += loss.item()\n",
    "                batch_val_acc += (y_hat_labels == y_val).type(torch.float32).mean().item()\n",
    "                        \n",
    "        # per batch loss\n",
    "        avg_batch_val_loss = batch_val_loss / len(dataloader_val)\n",
    "        per_epoch_val_loss.append(avg_batch_val_loss)\n",
    "                \n",
    "        avg_batch_val_acc = batch_train_acc / len(dataloader_val)\n",
    "        per_epoch_val_accuracy.append(avg_batch_val_acc)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        if verbose: print(f\"epoch: {epoch + 1}, train loss: {avg_batch_train_loss:.3f}, val loss: {avg_batch_val_loss:.3f}, train acc: {avg_batch_train_acc:.3f}, val acc: {avg_batch_val_acc:.3f}\")\n",
    "        \n",
    "        # early stopping\n",
    "        if epoch > 0 and per_epoch_val_loss[-1] > per_epoch_val_loss[-2]:\n",
    "            consec_increases =+ 1\n",
    "        \n",
    "        else: \n",
    "            consec_increases = 0\n",
    "        \n",
    "        if consec_increases == patience:\n",
    "            print(f'Stopped early at epoch {epoch + 1} because val loss increased for {consec_increases} consecutive epochs')\n",
    "    \n",
    "    results = {\"train_loss\": per_epoch_train_loss,\n",
    "               \"valid_loss\": per_epoch_val_loss,\n",
    "               \"train_accuracy\": per_epoch_train_accuracy,\n",
    "               \"valid_accuracy\": per_epoch_val_accuracy}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# set hyperparameters\n",
    "input_size = 46 # 1 x 46\n",
    "hidden_size_1 = 10\n",
    "hidden_size_2 = 5\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "LEARNING_RATE = 0.02\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size_1, hidden_size_2, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer(model, criterion, optimizer, dataloader_train, dataloader_val, epochs=num_epochs, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
